---
layout: post
title: "EM与HMM"
date: 2017-03-19 22:35:05
categories: MachineLearning
---
## EM算法
EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代由两步组成：E步，求期望（Expectation）；M步，求极大（Maximization）。

输入：观测变量数据$Y$，隐变量数据$Z$，联合分布$P(Y,Z\|\theta)$，条件分布$P(Z\|Y,\theta)$；

输出：模型参数$\theta$。

(1) 选择参数的初值$\theta^{(0)}$，开始迭代；

(2) E步：记$\theta^{(i)}$为第$i$次迭代参数$\theta$的估计值，在第$i+1$次迭代的E步，计算：

$$
Q(\theta,\theta^{(i)}) = E_{Z}\[\log P(Y,Z|\theta)|Y,\theta^{(i)}\]
$$
<!-- &= \sum_{Z}\log P(Y,Z\|\theta)P(Z\|Y,\theta^{(i)}) -->
<!-- 这里，$P(Z|Y,\theta^{(i)})$是在给定观测数据$Y$和当前的参数估计$\theta^{(i)}$下隐变量数据$Z$的条件概率分布；

(3) M步：求使得$Q(\theta,\theta^{(i)})$极大化的$\theta$，确定第$i+1$次迭代的参数的估计值$\theta^{(i+1)}$

$$
\theta^{(i+1)}=\arg\max_{\theta} Q(\theta, \theta^{(i)})
$$

(4) 重复第（2）和第（3）步，直到收敛。（Q函数是EM算法核心）

### Q函数
完全数据（Y和Z构成的数据）的对数似然函数$\log\,P(Y,Z|\theta)$关于在给定观测数据Y和当前参数$\theta^{(i)}$下对未观测数据Z的条件概率分布$P(Z|Y,\theta^{(i)})$的期望成为Q函数，即

$$
Q(\theta,\theta^{(i)})=E_{Z}[\log\,P(Y,Z|\theta)|Y,\theta^{(i)}]
$$

下面对EM算法作几点说明：

步骤（1） 参数的初值可以任意选择，但需注意EM算法对初值是敏感的；

步骤（2） E步求$Q(\theta,\theta^{(i)})$，每次迭代实际在求$Q$函数及其极大。

步骤（3） M步求$Q(\theta,\theta^{(i)})$的极大化，得到$\theta^{(i+1)}$。

###  -->