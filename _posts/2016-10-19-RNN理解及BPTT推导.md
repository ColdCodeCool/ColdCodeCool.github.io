---
layout: post
title: "Understand RNN and BPTT"
data: 2016-10-19 10:23:51
categories: TimeSeries
---
## 什么是RNN?
RNN(Recurrent Neural Network)背后的思想是利用序列化的信息.在一个传统神经网络中, 我们假设所有输入及输出内部之间是独立的.但是这个假设在许多任务当中是个非常错误的想法.比如, 如果你想预测一个句子的下一个单词, 你最好得知道它之前的单词是什么.RNNs之所以叫循环神经网络是因为他们对一个序列里的每一个元素都进行相同的任务操作, 从而得出的输出天然带有对之前状态的依赖. 另一种理解RNNs的方式是认为他们有记忆单元来获取目前已经计算过了的信息.理论上RNNs可以利用任意长序列的信息, 但在实际应用中他们只能被限制获取之前最近几步的信息. 一个典型的RNN长这样:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_nature.png)

上图展示了一个RNN被展开成一个full network. 举例来说, 如果我们有一个5个单词的句子, 那么网络将被展开成一个5层神经网络, 每一层对应一个单词. RNN背后的计算逻辑如下:

- $x_t$是时刻$t$的输入. 举例, $x_1$可以是一个代表一个句子第二个单词的one-hot vector.
- $s_t$是时刻$t$的隐含状态. 他是网络的记忆. $s_t$的计算基于之前的隐含状态和当前的输入:$s_t=f(Ux_t+Ws_{t-1})$. $f$函数通常是一个非线性的$tanh$或者$ReLU$.$s_{-1}$用来计算$s_0$,通常初始化为0.
- $o_t$是时刻$t$的输出. 举例, 如果我们要预测一个句子中的下一个单词, 那么$o_t$代表包含整个词汇表所以单词出现概率的向量.$o_t=softmax(Vs_t)$.

在这里有几件事情需要注意:

- 与传统神经网络在每层使用不同参数不同, 一个RNN在每一步或时刻都share相同的参数($U,V,W$). 这代表我们对每一步进行同样的操作, 只是利用了不同输入. 而且这样也极大地减少了需要学习的参数个数.
- 上面的图表的每一步都有输出, 但基于问题的不同, 这不是必须的. 比如, 当我们要预测一个句子的感情色彩时, 我们通常只关心最后的输出. 同样地, 我们也不一定在每一步都需要输入. RNN的主要feature是它的隐含状态, 通过它获取一个序列的信息.

### RNN能做什么
RNNs目前在多种NLP(Natural Language Processing)任务中获得巨大成功. 最常用的RNN类型是LSTM(Long Short Term Memory), 更擅长获取长时期的依赖. RNN能胜任的工作包含Language Modeling and generating text, Machine translation, Speech Recognition, Generating image description等.

RNN的其他扩展有Bidirectional RNNs, Deep RNNs, LSTM networks.