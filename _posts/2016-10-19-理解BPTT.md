---
layout: post
title: "Backpropagation through time and vanishing gradients"
data: 2016-10-19 14:02:29
categories: TimeSeries
---
## Backpropagation Through Time (BPTT)
RNN的基本计算逻辑:

$$
\begin{equation}
\begin{split}
s_t  &= tanh(Ux_t + Ws_{t-1})\\
\hat{y_t}  &= softmax(Vs_t)\\
\end{split}
\end{equation}
$$

我们这样定义loss:

$$
\begin{equation}
\begin{split}
& E_{t}(y_t,\hat{y_t})=-y_{t}\log \hat{y_t}\\
& E(y,\hat{y}) &= \sum_{t}E_{t}(y_t,\hat{y_t})\\
&= -\sum_{t}y_{t}\log \hat{y_t}\\
\end{split}
\end{equation}
$$

上式中, $y_t$是时刻$t$的正确单词, $\hat{y_t}$是我们的预测. 我们把整个sentence作为一个训练样本, 所以整个error就是每个time step sum.

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_error.png)

BPTT的目标是计算error相对于参数$U,V,W$的梯度, 然后用SGD学习更新这些参数. 就像累加每一步的error, 我们对每一个训练参数累加每一步的梯度:

$$
\frac{\partial E}{\partial W} = \sum_{t}\frac{\partial E_t}{\partial W}
$$

我们用链式求导规则来计算这些梯度, 以$E_3$为例, 我们来示例bptt如何work.

$$
\begin{equation}
\begin{split}
\frac{\partial E_3}{\partial V} &= \frac{\partial E_3}{\partial \hat{y_3}}\frac{\partial \hat{y_3}}{\partial V}\\
&=\frac{\partial E_3}{\partial \hat{y_3}}\frac{\partial \hat{y_3}}{\partial z_3}\frac{\partial z_3}{\partial V}\\
&=(\hat{y_3}-y_3)\otimes s_3\\
\end{split}
\end{equation}
$$

上式中, $z_3 = Vs_3$, $\otimes$是两个向量的外积(即对应位置元素相乘). 我们可以发现$\frac{\partial E_3}{\partial V}$仅仅依赖于$\hat{y_3},y_3,s_3$. 如果理解了这些, 计算$V$的梯度就仅仅是简单的矩阵乘法.

但是对于计算$\frac{\partial E_3}{\partial W}(以及$U$)与上面并不相同, 这点通过链式法则可以发现:

$$
\frac{\partial E_3}{\partial W}=\frac{\partial E_3}{\partial \hat{y_3}}\frac{\partial \hat{y_3}}{\partial s_3}\frac{\partial s_3}{\partial W}
$$

显然, $s_3=tanh(Ux_t+Ws_2)$仍然依赖于$s_2$, 而$s_2$又依赖于$W$和$s_1$...因此我们要通过链式法则计算:

$$
\frac{\partial E_3}{\partial W}=\sum_{k=0}^{3}\frac{\partial E_3}{\partial \hat{y_3}}\frac{\partial \hat{y_3}}{\partial s_3}\frac{\partial s_3}{\partial s_k}\frac{\partial s_k}{\partial W}
$$

我们把每一time step对于梯度的贡献累加起来. 在这里我们使用与标准backpropagation相同的trick, 即定义回传的delta vector. $\delta_{2}^{(3)}=\frac{\partial E_3}{\partial z_2}=\frac{\partial E_3}{\partial s_3}\frac{\partial s_3}{\partial s_2}\frac{\partial s_2}{\partial z_2}$ with $z_2=Ux_2 + Ws_1$.

一个简单的实现:

{% highlight python %}

def bptt(self, x, y):
	T = len(y)
	# Perform forward propagation
    o, s = self.forward_propagation(x)
    # We accumulate the gradients in these variables
    dLdU = np.zeros(self.U.shape)
    dLdV = np.zeros(self.V.shape)
    dLdW = np.zeros(self.W.shape)
    delta_o = o
    delta_o[np.arange(len(y)), y] -= 1.
    # For each output backwards...
    for t in np.arange(T)[::-1]:
        dLdV += np.outer(delta_o[t], s[t].T)
        # Initial delta calculation: dL/dz
        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))
        # Backpropagation through time (for at most self.bptt_truncate steps)
        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:
            # print "Backpropagation step t=%d bptt step=%d " % (t, bptt_step)
            # Add to gradients at each previous step
            dLdW += np.outer(delta_t, s[bptt_step-1])              
            dLdU[:,x[bptt_step]] += delta_t
            # Update delta for next step dL/dz at t-1
            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)
    return [dLdU, dLdV, dLdW]

{% endhighlight %}

### Vanishing gradient problem
在之前的链式法则求导中, 隐含状态之间的求导是一个jacobian matrix(因为是两个向量之间求导, 每个元素是pointwise derivatives). 而jacobian matrix的2范数的上界是1. 如果矩阵中值较小, 梯度值将指数级变化, 最终vanishing completely after a few time steps. 那么较远time steps的gradient contributions将趋于0, 以至于这些steps的states 将对参数学习毫无作用:即不能学习到long-range dependencies.

幸运的是, 有一些方法可以应付vanishing gradient problem. 比如恰当的initialization of $W$, 以及regularization, 用 ReLU代替$tanh$等. 而最常用的方法是LSTM或GRU, 下一篇文章将介绍.