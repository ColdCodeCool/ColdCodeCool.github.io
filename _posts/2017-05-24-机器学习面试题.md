---
layout: post
title: "机器学习面试题及解答"
date: 2017-05-24 13:54:06
categories: Interview
---
## 算法及原理类

Q1:偏差（bias）和方差之间的trade-off是什么？

- 偏差是由学习算法中错误的或过于简单的假设导致的。高偏差能导致算法忽略特征与目标输出的相关性，从而导致欠拟合（underfitting）。
- 方差是由学习算法中过于复杂的假设或模型导致的。这将导致你的算法对训练集中高度变化的数据过于敏感，从而导致模型过拟合（overfitting）。这样一来你的模型将携带太多的噪声导致模型在测试集上失效。

偏差-方差分解（bias-variance decomposition）通过将基于数据集中噪声的偏差，方差和由于噪声引起的不可避免的误差加在一起，从而分解了学习误差。本质上，如果你使模型更加复杂或增加更多变量，偏差会减小而方差会增大，因此我们需要在偏差方差上做权衡。

Q2:监督学习和无监督学习之间的区别是什么？

监督学习训练时要求带标签的数据，比如，为了进行分类（一种监督学习任务），你需要首先对数据打标签，从而你的训练模型才能对数据分类到其对应的标签组。

Q3:K最近邻与k-means聚类有什么区别？

k最近邻是一个监督学习算法，k-means是无监督学习算法。为使k最近邻算法工作，你需要带标签的数据从而将一个未被打标签的点分类。而k-means聚类仅仅需要一个未打标签的点集合以及一个门限：算法将不带标签的数据作为输入，并逐渐通过计算不同点之间的距离均值学习如何将它们聚类。

Q4:说明ROC曲线如何工作的。

ROC曲线是一个基于不同门限的真正率和假正率对比的图形化表示。它经常被用来作为权衡模型敏感性（真正率）与模型触发虚警概率（假正率）的代理。

Q5:定义精确率（precision）和召回率（recall)

召回率也就是真正率：即模型正确预测的正样本数量占应该被预测到的正样本数量的比例。精确率则是模型正确预测的正样本占实际被预测的正样本比例。举个例子，假如某个班级有男生80人，女生20人，共计100人。目标是找出所有女生，现在某人挑选出50个人，其中20个人是女生，另外还错误地把30个男生也当作女生挑选出来了。作为评估者的你来评价一下他的工作。

首先我们可以计算准确率（accuracy），其定义是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也就是损失函数是0-1损失时测试数据集上的准确率。结合实例，挑选者判定正确了20女+50男，而总人数是100，那么准确率是70%。

由准确率，我们的确可以在一些场合，从某种意义上得出一个分类器是否有效，但它并不是总能有效地评价一个分类器的性能。举例来说，google抓取了100个目标页面，而它索引中共有10，000，000个页面，随机抽一个页面，分类它是否目标页面？如果以accuracy作为指标来判定分类器性能，那我们可以选定一个无脑的分类器：它总是将抽取的页面判断为非目标页面，这样我们仍能得到9，999，900/10，000，000=99.999%的accuracy，显然这不合理。

那么我们需要引入precision，recall和f1-measure的概念，而在此之前要先定义TP，FP，FN，TN四种分类情况。在我们的例子中，TP=20，FP=30，FN=0，TN=50.

精确率公式是$P=\frac{TP}{TP+FP}$.
召回率$R=\frac{TP}{TP+FN}$.
F1=$\frac{2PR}{P+R}$，也即

$$
\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}
$$

Q6:什么是贝叶斯定理？

贝叶斯定理基于先验知识给出了一个事件的后验概率。

数学上来说，它表示了一个条件样本的真正率比上种群假正率与条件样本真正率之和。比如，你在流感测试后有60%的概率真的患流感，在所有患流感的人群中，有50%的概率测试会出错，并且整个人群中只有5%的概率患流感。那么你在一次流感测试中是否有60%的概率患流感。

答案是否定的。实际上你有

$$
\frac{0.6*0.05}{0.6*0.05+0.5*0.95}=0.0594
$$

的概率患流感。

Q7:为什么叫朴素贝叶斯？

朴素贝叶斯假设特征之间相互独立，互不影响。拿分类问题举例，对于样本$\textbf{x}$(假设是一个n维的向量)，我们要求$p(y\|\textbf{x})$，来估计所属类别。

根据贝叶斯公式，可以得到：

$$
p(y|\textbf{x})=p(\textbf{x}|y)p(y)/p(\textbf{x})
$$

右边的这个$p(\textbf{x}|y)$是我我们希望通过训练样本求得的。如果是普通的贝叶斯，那么这个$p(\textbf{x}|y)$会很大，与n的大小呈指数级增长；

如果是朴素贝叶斯，那么

$$
p(\textbf{x}|y)=p(x_1,x_2,\ldots|y)=p(x_1|y)*p(x_2|y)*\ldots
$$

这样以来，就可以把hypothesis的空间减小到与n的大小是线性关系了。

