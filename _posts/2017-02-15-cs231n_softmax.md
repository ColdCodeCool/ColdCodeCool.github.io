---
layout: post
title: "cs231n笔记2"
date: 2017-02-15 14:26:27
categories: cs231n
---
## Softmax
相对于logistic regression用于二分类，softmax相当于多分类版本的logistic regression。

# SGD and bells and whistles
## vanilla update：

$$
x += -learning_rate * dx
$$

## Momentum update
更新规则：

$$
v = mu * v - learning_rate * dx\\
x += v
$$

v是一个与dx相同形状的变量，初始化为0.mu则是动量，典型值是0.9，调整范围从0.5开始到0.99,[0.5,0.9,0.95,0.99].

## Nesterov momentum
更新规则：

$$
v_prev = v\\
v = mu * v - learning_rate * dx\\
x += -mu * v_prev + (1 + mu) * v.
$$


# per-parameter adaptive learning_rate methods
## Adagrad -- adaptive learning rate
规则是

$$
cache  += dx ** 2\\
x  += - learning\_rate * dx / (np.sqrt(cache) + eps)
$$

cache用来跟踪参数向量的二次方，然后用于更新参数步骤的element-wise的归一化参数向量。这样一来接收到大梯度值时，learning_rate将减小；接收到小梯度值时，learning_rate会变大。Adagrad的缺点在于，在深度学习中，单调的learning_rate往往过于激进以至于学习会过早终止。

## RMSprop
更新规则

$$
cache = decay_rate * cache + (1 - decay_rate) * dx**2\\
x += - learning_rate * dx / (np.sqrt(cache) + eps)
$$

该更新规则仅仅略微改动了Adagrad，以降低其单调地降低learning_rate带来的激进性。decay_rate是超参数，典型值[0.9,0.99,0.999]. 注意到RMSprop仍然根据每个weight参数的量级来调整learning_rate，但由于leaky cache的存在，learning_rate不再单调递减，从而不至于太激进。

## Adam
更新规则

$$
m = beta1*m + (1 - beta1)*dx\\
v = beta2*v + (1 - beta2)*(dx**2)\\
x += - learning_rate * m / (np.sqrt(v) + eps)
$$

是RMSprop的变种，使用了smooth version的gradient来替代raw gradient dx。参数推荐值，eps=1e-8，beta1=0.9, beta2=0.999。在实际应用中，adam被推荐为默认的参数更新规则并且比RMSprop稍好。但是，sgd+nesterov momentum往往也值得一试。在full版本的adam中还有一个bias correction，用来补偿初始几次迭代m和v被初始化为0。