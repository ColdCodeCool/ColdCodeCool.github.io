---
layout: post
title: "常用机器学习算法总结"
date: 2017-03-07 09:48:49
categories: MachineLearning
---
## 生成模型与判别模型
常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。

常见的生成模型有HMM、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。

## 偏差&方差
在统计学中，一个模型的好坏，是根据偏差和方差来衡量的：

- 偏差(bias):描述的是预测值的期望E与真实值Y之间的差距，偏差越大，越偏离真是数据。

$$
Bias[\hat{f}(x)] = E[\hat{f}(x)] - f(x)
$$

- 方差(variance):描述的是预测值的变化范围，离散程度，即其离期望值的距离。方差越大，预测值分布越分散。

模型的真实误差是两者之和：

$$
E[(y-\hat{f}(x))^{2}] = Bias[\hat{f}(x)]^{2} + Var[\hat{f}(x)] + \sigma ^{2}
$$

通常情况下，如果是小训练集，高偏差、低方差的分类器(例如，朴素贝叶斯)要比低偏差、高方差分类器(例如knn)的优势大，因为后者容易发生过拟合(overfitting)。然而，随着训练集的增长，模型对于原数据的预测能力就越好，偏差就会降低，此时低偏差、高方差的分类器就会渐渐地表现其优势(因为它们有较低的渐进误差)，而高偏差分类器这时已不足以提供准确的分类性能了。

当然，也可以将其理解为生成模型(如朴素贝叶斯)和判别模型(如knn)的一个区别。

# 常见算法优缺点
### 1.朴素贝叶斯
朴素贝叶斯属于生成式模型（关于生成模型和判别式模型的区别，主要在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设，朴素贝叶斯分类器的收敛速度将快于判别模型如罗辑回归，所以只需要较少的训练数据即可。即使条件独立性假设不成立，朴素贝叶斯在实践中的表现仍然很出色，而它的缺点主要是不能学习特征间的相互作用。

优点：

- 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。
- 适合增量式训练，即可以实时地对新增的样本进行训练；
- 对缺失数据不太敏感，算法也比较简单，常用于文本分类或欺诈检测。

缺点：

- 需要计算先验概率；
- 分类决策存在错误率；
- 对输入数据的表达形式很敏感。

### 2.罗辑回归
属于判别式模型，同时伴有很多模型正则化的方法（l0,l1,l2)，而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。

优点：

- 实现简单，广泛地应用于工业问题上；
- 分类时计算量非常小，速度很快，存储资源低；
- 便利地观测样本概率分布；
- 对罗辑回归而言，多重共线性并不是问题，它可以结合l2正则化来解决该问题。

缺点：

- 当特征空间很大时，罗辑回归的性能不是很好；
- 容易欠拟合，一般准确度不太高；
- 不能很好地处理大量多类特征或变量；
- 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；
- 对于非线性特征，需要进行转换。

### 3.k近邻
主要过程：

- 计算训练样本和测试样本中每个样本点的距离；
- 对上面所有的距离值进行排序（升序）；
- 选前k个最小距离的样本；
- 根据这k个样本的标签进行投票，得到最后的分类类别。

如何选择一个最佳的k值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。

优点：

- 理论成熟，思想简单，即可以用来做分类也可以用来做回归；
- 可用于非线性分类；
- 训练实践复杂度为O（n）；
- 对数据没有假设，准确度高，对outlier不敏感。

缺点：

- 计算量大；
- 样本不平衡时，预测偏差比较大。
- knn每一次分类都会重新进行一次全局运算；
- k值大小的选择。

### 4. 决策树
决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。

决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。

信息熵的计算公式如下:

$$
H = -\sum_{i=1}^{n}p(x_i)log_{2}p(x_i)
$$

其中的n代表有n个分类类别（比如假设是二类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。

现在选中一个属性xixi用来进行分枝，此时分枝规则是：如果xi=vxi=v的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵H’ =p1 H1+p2 H2,则此时的信息增益ΔH = H - H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性。

优点：

- 易于理解和解释，可以可视化分析，容易提取出规则；
- 比较适合处理有缺失属性的样本；
- 能处理不相关的特征；
- 可以同时处理标称型和数值型数据；
- 测试数据集时，运行速度比较快；
- 决策树可以很好地扩展到大型数据库中，同时它的大小独立于数据库大小。

缺点：

- 容易发生过拟合（随机森林可以很大程度上减少过拟合）；
- 忽略了数据之间的相关性；
- 对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只要是使用了信息增益，都有这个缺点，如RF）。

### 5.支持向量机
支持向量机，一个经久不衰的算法，高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。

优点：
- 可以解决高维问题，即大型特征空间；
- 能够处理非线性特征的相互作用；
- 无需依赖整个数据；
- 可以提高泛化能力。

缺点：
- 当观测样本很多时，效率并不高；
- 难以找到合适的核函数；
- 对缺失数据敏感。

对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：

- 如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；
- 如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；
- 如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。

对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。

### 6.K-means
优点：

- 算法简单，容易实现
- 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法通常局部收敛。；
- 算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。

缺点：

- 对数据类型要求较高，适合数值类型数据；
- 可能收敛到局部最小值，在大规模数据上收敛较慢；
- K值比较难以选取；
- 对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类效果；
- 不适合于发现非凸面形状的簇，或者大小差别很大的簇；
- 对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。
