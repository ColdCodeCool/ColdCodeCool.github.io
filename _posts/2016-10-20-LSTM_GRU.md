---
layout: post
title: "Understand LSTM and GRU"
data: 2016-10-20 11:27:26
categories: TimeSeries
---
## LSTM Networks
我们使用LSTM来combat vanishing gradients through a gating mechanism. 我们来看一个LSTM如何计算$s_t$:

$$
\begin{equation}
\begin{split}
&i &= \sigma(x_{t}U^{i}+s_{t-1}W^{i})\\
&f &= \sigma(x_{t}U^{f}+s_{t-1}W^{f})\\
&o &= \sigma(x_{t}U^{o}+s_{t-1}W^{o})\\
&g &= tanh(x_{t}U^{g}+s_{t-1}W^{g})\\
&c_t &= c_{t-1}\circ f+g\circ i\\
&s_t &= tanh(c_t)\circ o\\
\end{split}
\end{equation}
$$

上面公式看似复杂, 但其实一个LSTM layer也只是另外一种计算隐含状态的方式. 如果把LSTM当做一个黑盒, 其内部计算如下:

- $i,f,o$分别代表input, forget和output gates. 注意到他们具有相同的函数形式, 而仅仅只是使用不同的参数矩阵. 因为sigmoid函数把这些向量的值squashes到0到1之间, 所以被称作门. input gate决定了你想让多少对应当前的输入计算出的state通过门. forget gate决定了你想让多少之前的state值通过此门. output gate决定你想让多少internal state值expose to the external network. All the gates have the same dimensions $d_s$, the size of your hidden state.
- $g$ is a “candidate” hidden state that is computed based on the current input and the previous hidden state. It is exactly the same equation we had in our vanilla RNN, we just renamed the parameters $U$ and $W$ to $U^g$ and $W^g$. However, instead of taking g as the new hidden state as we did in the RNN, we will use the input gate from above to pick some of it.
- $c_t$ is the internal memory of the unit. It is a combination of the previous memory $c_{t-1}$ multiplied by the forget gate, and the newly computed hidden state $g$, multiplied by the input gate. Thus, intuitively it is a combination of how we want to combine previous memory and the new input. We could choose to ignore the old memory completely (forget gate all 0’s) or ignore the newly computed state completely (input gate all 0’s), but most likely we want something in between these two extremes.
- Given the memory $c_t$, we finally compute the output hidden state $s_t$ by multiplying the memory with the output gate. Not all of the internal memory may be relevant to the hidden state used by other units in the network.

### 结合Chris Olah的博客理解LSTM
标准RNN的重复模块如图:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_single.png)

LSTMs同样有如上图的链式结构, 但是重复模块的内部有不同的结构. 相比于上图内部结构的单层网络层, LSTMs的有四层:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm.png)

对于上图的内部结构:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm1.png)

### LSTMs背后的核心思想
LSTMs的关键在于cell state, 即下面结构图中顶部的水平线. cell state可以看成一条传送带. 随着整条链式结构往前走, 仅仅有一些线性交互.

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm2.png)

LSTM可以通过一种门结构来控制移除或添加信息到cell state. 门结构可以选择性地让信息通过, 它们由一个sigmoid neural net layer和一个pointwise multiplication操作组成:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm3.png)

一个LSTM有三个这种门, 来保护和控制cell state.

### Step-by-Step LSTM walk through
下面我们将一步步解剖LSTM的内部结构.

##### 第一步:决定从cell state中丢掉哪些信息. 
这个决定由一个sigmoid layer来做出, 即"forget gate layer". "f gate"接收$h_{t-1}$和$x_t$, 并输出一个0到1之间的数到$c_{t-1}$. 输出1代表完全保留, 输出0代表完全丢弃.

回到我们之前用语言模型基于之前单词预测下一单词的例子.在这个问题中, cell state的潜在含义可能是包括当前主语的词性, 所以下一单词可能使用正确的代词. 而当我们看到新的主语时, 我们想忘记之前主语的词性(从而不至于干扰我们这次如何选择代词).

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm4.png)

##### 第二步:决定在cell state中储存什么样的新信息
这包含两个方面. 第一, 一个称作"input gate"的sigmoid layer决定我们将更新哪些值. 第二, 一个tanh layer创造出一个新的候选值向量, $\tilde{C_t}$, 将被加到state中去. 然后, 我们将这二者结合生成一个state的更新.

在我们的language model中, 我们想要把新的主语的词性添加到cell state中, 来代替我们将要forgetting的原先预测的主语词性. 

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm5.png)

现在我们就可以将old cell state $C_{t-1}$更新到新的cell state $C_t$. 之前的步骤已经决定了做什么, 我们只需要按部就班.

在language model的例子中, 上面的步骤代表了我们决定忘记那些关于老的主语的词性并且添加新的信息.

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm6.png)

#####决定输出什么.
这个输出将基于我们的cell state, 但是略有改变. 首先, 我们run一个决定cell state的什么部分将要输出的sigmoid layer. 然后, 将其与tanh的cell state想成, 以此来决定我们输出什么部分.

对于language model的例子, 如果遇到了一个主语, model将要输出一个与动词相关的信息, 也是主语之后原本接下来应该输出的内容. 例如, model将根据主语是单数还是复数, 来输出接下来的动词应该是什么形式.

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm7.png)

## Variants on LSTM
一, 加peephole connections.

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm8.png)

二, Gated Recurrent Unit.

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm9.png)