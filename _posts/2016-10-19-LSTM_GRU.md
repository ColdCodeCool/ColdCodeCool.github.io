---
layout: post
title: "Understand LSTM and GRU"
data: 2016-10-19 16:50:27
categories: TimeSeries
---
## LSTM Networks
我们使用LSTM来combat vanishing gradients through a gating mechanism. 我们来看一个LSTM如何计算$s_t$:

$$
i = \sigma(x_{t}U^{i}+s_{t-1}W^{i})\\
f = \sigma(x_{t}U^{f}+s_{t-1}W^{f})\\
o = \sigma(x_{t}U^{o}+s_{t-1}W^{o})\\
g = tanh(x_{t}U^{g}+s_{t-1}W^{g})\\
c_t = c_{t-1}\circ+g\circ i\\
s_t = tanh(c_t)\circ o\\
$$

上面公式看似复杂, 但其实一个LSTM layer也只是另外一种计算隐含状态的方式. 如果把LSTM当做一个黑盒, 其内部计算如下:

- $i,f,o$分别代表input, forget和output gates. 注意到他们具有相同的函数形式, 而仅仅只是使用不同的参数矩阵. 因为sigmoid函数把这些向量的值squashes到0到1之间, 所以被称作门. input gate决定了你想让多少对应当前的输入计算出的state通过门. forget gate决定了你想让多少之前的state值通过此门. output gate决定你想让多少internal state值expose to the external network. All the gates have the same dimensions $d_s$, the size of your hidden state.
- $g$ is a “candidate” hidden state that is computed based on the current input and the previous hidden state. It is exactly the same equation we had in our vanilla RNN, we just renamed the parameters $U$ and $W$ to $U^g$ and $W^g$. However, instead of taking g as the new hidden state as we did in the RNN, we will use the input gate from above to pick some of it.
- $c_t$ is the internal memory of the unit. It is a combination of the previous memory $c_{t-1}$ multiplied by the forget gate, and the newly computed hidden state $g$, multiplied by the input gate. Thus, intuitively it is a combination of how we want to combine previous memory and the new input. We could choose to ignore the old memory completely (forget gate all 0’s) or ignore the newly computed state completely (input gate all 0’s), but most likely we want something in between these two extremes.
- Given the memory $c_t$, we finally compute the output hidden state $s_t$ by multiplying the memory with the output gate. Not all of the internal memory may be relevant to the hidden state used by other units in the network.

### 结合Chris Olah的博客理解LSTM
标准RNN的重复模块如图:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_single.png)

LSTMs同样有如上图的链式结构, 但是重复模块的内部有不同的结构. 相比于上图内部结构的单层网络层, LSTMs的有四层:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm.png)

对于上图的内部结构:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm1.png)

### LSTMs背后的核心思想
LSTMs的关键在于cell state, 即下面结构图中顶部的水平线. cell state可以看成一条传送带. 随着整条链式结构往前走, 仅仅有一些线性交互.

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm2.png)

LSTM可以通过一种门结构来控制移除或添加信息到cell state. 门结构可以选择性地让信息通过, 它们由一个sigmoid neural net layer和一个pointwise multiplication操作组成:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/rnn_lstm3.png)

一个LSTM有三个这种门, 来保护和控制cell state.

### Step-by-Step LSTM walk through
第一步, 决定从cell state中丢掉哪些信息. 这个决定由一个sigmoid layer来做出, 即"forget gate layer".