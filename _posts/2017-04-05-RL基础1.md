---
layout: post
title: "强化学习基础"
date: 2017-04-05 16:22:41
categories: ReinforcementLearning
---
## Reinforcement Learning Summary
Summary:

- There is no supervisor, only a reward signal.
- Reinforcement Learning (RL) is concerned with goal-directed learning and decision-making.
- In RL an agent learns from experiences it gains by interacting with environment. In Supervised Learning we cannot affect the environment.
- In RL rewards (feedback) are often delayed (not instantaneous) in time and the agent tries to maximise a long-term goal. For example, one may need to make seemingly suboptimal moves to reach a winning position in a game.
- An agent interacts with the environment via states, actions and rewards.
- Time really matters(sequential, non i.i.d data)
- Agent's actions affect the subsequent data it receives.

## Fully observable environments
Full observability: agent directly observes environment state

$$
O_t = S_{t}^{a} = S_{t}^{e}
$$

Agent state = environment state = information state, formally, this is a Markov decision process (MDP).

## Partially observable environments
Partial observability: 
agent indirectly observes environment:

- A robot with camera vision isn't told its absolute location.
- A trading agent only observes current prices.
- A poker playing agent only observes public cards.

Now agent state $\neq$ environment state, formally this is a partially observable MDP.

Agent must construct its own state representation $S_{t}^{a}$,e.g

- Complete history:$S_{t}^{a}=H_{t}$.
- Beliefs of environment state:$S_{t}^{a}=(P[S_{t}^{e}=s^1,\ldots,P[S_{t}^{e}=s^n]])$.
- Recurrent neural network:$S_{t}^{a}=\sigma(S_{t-1}^{a}W_{s} + O_{t}W_o)$.

## Major components of an RL agent
An RL agent may include one or more these components:

- Policy: agent's behaviour function
- Value function: how good is each state and/or action
- Model: agent's representation of the environment

### Policy

- A policy is the agent's behaviour
- It is a map from state to action, e.g.
- Determinstic policy: $a = \pi(s)$
- Stochastic policy:$\pi(a|s)= P[A=a|S=s]$

### Value function

- Value function is a prediction of future reward
- Used to evaluate the goodness/badness of states
- And therefore to select between actions, e.g.

$$
v_{\pi}(s) = E_{\pi}[R_t+\gamma R_{t+1}+\gamma^{2}R_{t+2}+\ldots|S_{t}=s]
$$

# Learning and Planning
Two fundamental problems in sequential decision making

- Reinforcement Learning:
	- The environment is initially unknown
	- The agent interacts with the environment
	- The agent improves its policy
- Planning
	- A model of the environment is known
	- The agent performs computations with its model(without any external interaction)
	- The agent improves its policy
	- 

# Why discount?
Most Markov reward and decision processes are discounted?Why?

- Mathematically convenient to discount rewards
- Avoids infinite returns in cyclic Markov processes
- Uncertainty about the future may not be fully represented
- If the reward is financial, immediate rewards may earn more interest than delayed rewards
- Animal/human behaviour shows preference for immediate reward
- It is sometimes possible to use undiscounted Markov reward processes (i.e. $\gamma=1.0$), e.g. if all sequences terminate.

## MDP和强化学习的关系
在强化学习中仍然需要假设存在一个MDP:

- A set of states $s\in \mathcal{S}$
- A set of actions (per state) $\mathcal{A}$
- A model T(s,a,s') (transition function)
- A reward function R(s,a,s')

1. 仍然需要找到一个策略$\pi(s)$

2. 新变数(new twist):不知道T或R

- i.e. we don't know which states are good or what the actions do
- Must actually try actions and states out to learn

3. Offline (MDPs) vs. Online (RL)

## Q-value iteration

- Value iteration:find successive (depth-limited) values
	- Start with $V_{0}(s)=0$, which we know is right
	- Given $V_{k}$, caculate the depth k+1 values for all states:
	\begin{align}
	V_{k+1}(s)\leftarrow \max_{a}\sum_{s'}T(s,a,s')\[R(s,a,s') + \gamma V_{k}(s')\]
	\end{align}
- But Q-values are more useful, so compute them instead:
	- Start with $Q_{0}(s,a)=0$, which we know is right
	- Given $Q_{k}$, caculate the depth k+1 q-values for all q-states.

Q-learning特点：

- Amazing result：Q-learning converges to optimal policy, even if you are acting suboptimally!
- This is called off-policy learning
- Caveats(警告或附加说明):
	- You have to explore enough(需要足够的搜索)
	- You have to eventually make the learning rate small enough
	- ...but not decrease it too quickly
	- Basically, in the limit, it doesn't matter how you selects actions(!)

## Value iteration
伪代码：

- Assign each state a random value
- For each state, calculate its new utility based on its neighbor's values
- Update each state's utility based on the calculation above
- if no value changes by more than a tolerance, stop;

基于bellman equation的更新方程：
\begin{align}
V_{k+1}(s)\leftarrow \max_{a}\sum_{s'}T(s,a,s')\[R(s,a,s')+\gamma V_{k}(s')\]
\end{align}

Value iteration的缺点:

- It's slow. $O(S^{2}A)$ per iteration.
- The "max" at each state rarely changes
- The policy often coverges long before the values

## Policy iteration
Alternative approach for optimal values:

- Step 1:Policy evaluation:calculate utilities for some fixed policy (not optimal) until convergence
- Step 2:Policy improvement:update policy using one-step look-ahead with resulting coverged (but not optimal) utilities as future values
- Repeat steps until policy converges

Evaluation:For fixed current policy $\pi$, find values with policy iteration:Iterate until values converge:
\begin{align}
V_{k+1}^{\pi_{i}}(s)\leftarrow \sum_{s'}T(s,\pi_{i}(s),s')\[R(s,\pi_{i},s')+\gamma V_{k}^{\pi_{i}}(s')\]
\end{align}
Improvement:For fixed values, get a better policy using policy extraction:One-setp look-ahead:
\begin{align}
\pi_{i+1}(s)=\argmax_{a}\sum_{s'}T(s,a,s')\[R(s,a,s')+\gamma V^{\pi_{i}}(s')\]
\end{align}

# Value iteration 与 policy iteration对比

- Both value iteration and policy iteration compute the same thing (all optimal values)
- In value iteration:
	- Every iteration updates both the values and (implicitly) the policy
	- We don't track the policy, but taking the max over actions impliciyly recomputes it
- In policy iteration:
	- We do several passes that update utilities with fixed policy(each pass is fast because we consider only one action, not all of them)
	- After the policy is evaluated, a new policy is chosen(slow like a value iteration pass)
	- The new policy will be better (or we're done)
- Both are dynamic programs for solving MDPs

## Generalizing across states

- Basic Q-learning keeps a table of all q-values
- In realistic situations, we cannot possibly learn about every single state
	- Too many states to visit them all in training
	- Too many states to hold the q-tables in memory
- Instead, we want to generalize:
	- Learn about some small number of training states from experience
	- Generalize that experience to new, similar situations
	- This is a fundamental idea in machine learning,and we'll see it over and over again.

#### solution to generalize: feature-based representations

- Solution:describe a state using a vector of features

#### Linear value functions

- Using a feature representation, we can write a q function (or value function) for any state using a few weights:
\begin{align}
V(s) = w_{1}f_{1}(s)+w_{2}f_{2}(s)+\ldots+w_{n}f_{n}(s)\\
Q(s,a) = w_{1}f_{1}(s,a)+w_{2}f_{2}(s,a)+\ldots+w_{n}f_{n}(s,a)
\end{align}

- Advantage: our experience is summed up in a few powful numbers
- Disadvantage: states may share features but actually be very different in values.
