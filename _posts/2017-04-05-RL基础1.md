---
layout: post
title: "强化学习基础"
date: 2017-04-05 16:22:41
categories: ReinforcementLearning
---
## Reinforcement Learning Summary
Summary:

- There is no supervisor, only a reward signal.
- Reinforcement Learning (RL) is concerned with goal-directed learning and decision-making.
- In RL an agent learns from experiences it gains by interacting with environment. In Supervised Learning we cannot affect the environment.
- In RL rewards (feedback) are often delayed (not instantaneous) in time and the agent tries to maximise a long-term goal. For example, one may need to make seemingly suboptimal moves to reach a winning position in a game.
- An agent interacts with the environment via states, actions and rewards.
- Time really matters(sequential, non i.i.d data)
- Agent's actions affect the subsequent data it receives.

## Fully observable environments
Full observability: agent directly observes environment state

$$
O_t = S_{t}^{a} = S_{t}^{e}
$$

Agent state = environment state = information state, formally, this is a Markov decision process (MDP).

## Partially observable environments
Partial observability: agent indirectly observes environment:

- A robot with camera vision isn't told its absolute location.
- A trading agent only observes current prices.
- A poker playing agent only observes public cards.

Now agent state $\neq$ environment state, formally this is a partially observable MDP.

Agent must construct its own state representation $S_{t}^{a}$,e.g

- Complete history:$S_{t}^{a}=H_{t}$.
- Beliefs of environment state:$S_{t}^{a}=(P[S_{t}^{e}=s^1,\ldots,P[S_{t}^{e}=s^n]])$.
- Recurrent neural network:$S_{t}^{a}=\sigma(S_{t-1}^{a}W_{s} + O_{t}W_o)$.

## Major components of an RL agent
An RL agent may include one or more these components:
- Policy: agent's behaviour function
- Value function: how good is each state and/or action
- Model: agent's representation of the environment

### Policy

- A policy is the agent's behaviour
- It is a map from state to action, e.g.
- Determinstic policy: $a = \pi(s)$
- Stochastic policy:$\pi(a|s)= P[A=a|S=s]$

### Value function

- Value function is a prediction of future reward
- Used to evaluate the goodness/badness of states
- And therefore to select between actions, e.g.

$$
v_{\pi}(s) = E_{\pi}[R_t+\gamma R_{t+1}+\gamma^{2}R_{t+2}+\ldots|S_{t}=s]
$$

# Learning and Planning
Two fundamental problems in sequential decision making

- Reinforcement Learning:
	- The environment is initially unknown
	- The agent interacts with the environment
	- The agent improves its policy
- Planning
	- A model of the environment is known
	- The agent performs computations with its model(without any external interaction)
	- The agent improves its policy
	- 

