---
layout: post
title: "强化学习基础"
date: 2017-04-05 16:22:41
categories: ReinforcementLearning
---
## Reinforcement Learning Summary
Summary:

- There is no supervisor, only a reward signal.
- Reinforcement Learning (RL) is concerned with goal-directed learning and decision-making.
- In RL an agent learns from experiences it gains by interacting with environment. In Supervised Learning we cannot affect the environment.
- In RL rewards (feedback) are often delayed (not instantaneous) in time and the agent tries to maximise a long-term goal. For example, one may need to make seemingly suboptimal moves to reach a winning position in a game.
- An agent interacts with the environment via states, actions and rewards.
- Time really matters(sequential, non i.i.d data)
- Agent's actions affect the subsequent data it receives.

## Fully observable environments
Full observability: agent directly observes environment state

$$
O_t = S_{t}^{a} = S_{t}^{e}
$$

Agent state = environment state = information state, formally, this is a Markov decision process (MDP).

## Partially observable environments
Partial observability: 
agent indirectly observes environment:

- A robot with camera vision isn't told its absolute location.
- A trading agent only observes current prices.
- A poker playing agent only observes public cards.

Now agent state $\neq$ environment state, formally this is a partially observable MDP.

Agent must construct its own state representation $S_{t}^{a}$,e.g

- Complete history:$S_{t}^{a}=H_{t}$.
- Beliefs of environment state:$S_{t}^{a}=(P[S_{t}^{e}=s^1,\ldots,P[S_{t}^{e}=s^n]])$.
- Recurrent neural network:$S_{t}^{a}=\sigma(S_{t-1}^{a}W_{s} + O_{t}W_o)$.

## Major components of an RL agent
An RL agent may include one or more these components:

- Policy: agent's behaviour function
- Value function: how good is each state and/or action
- Model: agent's representation of the environment

### Policy

- A policy is the agent's behaviour
- It is a map from state to action, e.g.
- Determinstic policy: $a = \pi(s)$
- Stochastic policy:$\pi(a|s)= P[A=a|S=s]$

### Value function

- Value function is a prediction of future reward
- Used to evaluate the goodness/badness of states
- And therefore to select between actions, e.g.

$$
v_{\pi}(s) = E_{\pi}[R_t+\gamma R_{t+1}+\gamma^{2}R_{t+2}+\ldots|S_{t}=s]
$$

# Learning and Planning
Two fundamental problems in sequential decision making

- Reinforcement Learning:
	- The environment is initially unknown
	- The agent interacts with the environment
	- The agent improves its policy
- Planning
	- A model of the environment is known
	- The agent performs computations with its model(without any external interaction)
	- The agent improves its policy
	- 

# Why discount?
Most Markov reward and decision processes are discounted?Why?

- Mathematically convenient to discount rewards
- Avoids infinite returns in cyclic Markov processes
- Uncertainty about the future may not be fully represented
- If the reward is financial, immediate rewards may earn more interest than delayed rewards
- Animal/human behaviour shows preference for immediate reward
- It is sometimes possible to use undiscounted Markov reward processes (i.e. $\gamma=1.0$), e.g. if all sequences terminate.

## MDP和强化学习的关系
在强化学习中仍然需要假设存在一个MDP:

- A set of states $s\in \mathcal{S}$
- A set of actions (per state) $\mathcal{A}$
- A model T(s,a,s') (transition function)
- A reward function R(s,a,s')

1. 仍然需要找到一个策略$\pi(s)$

2. 新变数(new twist):不知道T或R

- i.e. we don't know which states are good or what the actions do
- Must actually try actions and states out to learn

3. Offline (MDPs) vs. Online (RL)

## Q-value iteration

- Value iteration:find successive (depth-limited) values
	- Start with $V_{0}(s)=0$, which we know is right
	- Given $V_{k}$, caculate the depth k+1 values for all states:
	\begin{align}
	V_{k+1}(s)\leftarrow \max_{a}\sum_{s'}T(s,a,s')\[R(s,a,s') + \gamma V_{k}(s')\]
	\end{align}
- But Q-values are more useful, so compute them instead:
	- Start with $Q_{0}(s,a)=0$, which we know is right
	- Given $Q_{k}$, caculate the depth k+1 q-values for all q-states.
	