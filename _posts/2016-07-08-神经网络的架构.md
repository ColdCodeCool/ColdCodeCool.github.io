---
layout: post
title: "神经网络的架构"
date: 2016-07-08 15:12:19
categories: Deeplearning
---
## Architecture
假设我们有下面的一个网络:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/archi.png)


我们把第一层叫`input layer`(输入层), 中间那一层叫`hidden layer`, 最后一层叫`output layer`. input layer和output layer都相当容易理解, 但hidden layer听起来可能有些神秘, 当我第一次看到这个词时, 甚至觉得带有某种深刻的哲学或数学意义. 但事实上, 它仅仅就是"不是输入层也不是输出层的网络层". 上图当中仅有一个隐藏层, 事实上其他网络可能有多个隐藏层. 由于历史的原因, 上图的多层网络有时被叫做 multilayer perceptrons或MLPs, 而忽略了它们是由sigmoid neurons构成的事实. 

输入和输出层往往是直观的, 例如, 我们要判断一个手写体的图片是否是数字"9". 一个自然的联想是将图片的像素密度编码为输入神经元的输入, 若图片是一张64 x 64的灰度图, 那么我们有4096=64 x 64个输入神经元, 像素密度在0到1之间. 输出层只有一个神经元, 当输出大于0.5时, 输入图像是"9", 否则不是"9".

虽然输入和输出层的设计是简单直观的, 但是隐藏层的设计相当有技巧. 隐藏层神经元的个数选择直接影响到计算量的大小, 但是好在研究者们开发出许多如何设计隐藏层的技巧. 直到现在, 我们都在讨论神经元的输出作为下一层神经元输入的网络, 我们称这种网络为前馈网络, feedforward neural network. 即网络中没有循环, 但其他类型的网络如recurrent neural network当中是存在这些循环的.

## A simple network to classify handwritten digits
我们使用一个三层网络:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/three.png)

上图网络的输入层由一张28 x 28个像素的图片编码构成, 每个像素的灰度值由0到1, 0代表白色, 1代表黑色. 隐藏层由15个神经元构成, 输出层由10个神经元构成, 如果第一个神经元被激发, 即输出约为1, 那么我们将这个图片识别为0.

你也许会思考这样一个问题, 为什么我们要使用10个神经元作为输出层. 毕竟, 这个网络的目标是告诉我们哪个数字代表了输入的图像. 一个看起来自然的方式是我们只需要4个二元输出神经元就可以表征所有10个数字, 因为4个神经元可以表示16种结果. 那么为什么我们还需要使用10个神经元. 事实上, 这个问题可以由实验证明, 我们同时使用两种方式来设计网络, 结果是使用10个输出神经元的效果要比4个更好. 

为了理解这个现象, 我们考虑使用10个神经元的结构. 我们考虑第一个输出神经元, 它根据隐藏层的加权输出来判断输入图片是否为"0". 那么隐藏层神经元是干嘛的? 好吧, 我们假设隐藏层的第一个神经元负责检测一张图片是否出现下面图片的部分:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/part.png)

那么, 隐藏层第一个神经元要做的事情就是赋予与上图位置重叠的像素极高的权重, 而对输入的其他像素位置赋予很小的权值. 类似地, 我们假设第二, 三, 四个隐藏层神经元是要检测如下图位置的像素是否存在:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/hidden.png)

正如你猜地那样, 这四个图像组合起来就是数字"0". 所以, 如果上面四个隐藏层神经元都被激发, 那么我们可以总结出输入的图像就是数字"0". 当然, 上面的四个证据组合并不唯一指向数字"0". 我们可以合法地得到更多其他的数字"0"的图像组合. 但是, 在此例子中, 我总结得到输入为数字"0"显然是安全的.

如果这就是神经网络的工作模式, 那么我们可以给出为什么10个输出神经元比4个要更好的解释. 假设我们只有4个输出神经元, 那么第一个输出神经元将要尝试决定输入图像的哪一个bit最重要, 这对于仅有4个输出神经元的网络是十分困难的.

## Learning with gradient descent
现在我们已经设计了一个神经网络, 那么它将如何通过学习来识别手写数字呢? 首先, 我们需要一个训练数据集, 这里我们使用MNIST data set. 数据集包含两部分, 第一部分包含60000张手写数字的图片, 作为训练集. 另一部分包含10000张图像, 作为测试集, 这些图像都是28 x 28的灰度图.

我们将训练输入用$x$来表示, 并且将$x$表示为一个$28 x 28 = 784$维的向量. 我们将输出表示为$y = y(x)$, $y$是一个10维的向量. 例如, 对于一个特定的训练图像, $x$, 对应数字6, 那么$y(x)=(0,0,0,0,0,0,1,0,0,0)^{T}$表示了相应的输出.

我们想实现这样一种算法, 即能够使我们找到合适的权值和偏置值来让神经网络对于所有训练输入$x$的输出逼近$y(x)$. 为了定量我们如何趋近这一目标, 我们定义一个cost function:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/cost.png)

在上式中, $w$表示网络权值的集合, $b$表示偏置集合, $n$是训练集的规模, $a$是当输入为$x$时的网络输出向量, 求和是针对所有训练输入. 我们的目标是使$C(w,b)$越小越好, 下面我们将使用梯度下降法(gradient descent)来达到这个目标.

假设$C$是一个带有$m$个变量的函数, $v_{1},...,v_m$. 那么$\Delta C$表示由权值$w$, 偏置值$b$构成的向量$v$的微小变化$\Delta v = (\Delta v_{1},...,\Delta v_m)^{T}$引起的cost变化:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/costchange.png)

cost的梯度:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/costchange.png)

对仅有两个变量的实例, 我们可以选择:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/changeforv.png)

这给了我们一种使$C$趋向最小值的方法, 通过不断重复使用对于$v$的更新规则:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/updatev.png)

因此我们可以将这个更新v的规则定义为`梯度下降法`(gradient descent algorithm). 它给我们提供了一种方法, 通过微小改变v的值来找到$C$的最小值. 一定程度上, 梯度下降法是搜索最小值的最优策略. 我们控制$v$的移动步长为$\|\Delta v\| = \epsilon$, $\epsilon$为选取的固定正数. 可以证明$\Delta v$使$\nabla C \centerdot \Delta v$减小的最优选择是$\Delta v = -\eta \nabla C$.

具体的更新策略是用$w_k$和$b_l$来代替$v_j$, 可以得到:


![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/weights.png)


重复使用这个更新策略, 我们就能使cost function渐渐趋向最小值. 但在实际应用梯度下降法时, 我们还会遇到一些挑战. 我们注意到cost function的形式: $C = \frac{1}{n}\sum_{x}C_x$, 是对单个训练样本的平均:$C_{x} = \frac{\| y(x) - a\|^2}{2}$. 实际上, 我们计算cost的梯度$\nabla C$时需要单独对每个训练样本计算cost的梯度$\nabla C_x$, 然后才有$\nabla C = \frac{1}{n}\sum_x \nabla C_x$. 然而训练样本规模往往很大, 这样的训练方法非常耗时, 学习过程很慢.


这时我们用随机梯度下降(stochastic gradient descent)来加速学习, 具体地就是随机采样整个训练输入, 然后在采样到的小规模训练样本上学习. 然后用得到的平均cost来近似总体cost:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/stochastic.png)

对$w_k$, $b_k$来说:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/wb.png)

完成一次mini-batch的训练之后, 再对另一组mini-batch训练, 直到遍历完整个训练集, 然后我们就完成了一次epoch训练, 再进行下一个epoch的训练. 另外我们有必要注意cost function和mini-batch更新参数的规模变化, 上式中对cost乘以因子$\frac{1}{n}$. 有时我们忽略这个因子, 直接对每个训练样本cost求和, 这一点对训练样本规模未知(训练样本实时产生)的情况非常有用. 同样地, 对mini-batch更新时我们有时也忽略因子$\frac{1}{m}$. 从概念上这并不改变什么, 因为我们可以等效地调整$\eta$来实现. 举例来说, 假设总训练样本$n = 60000$, 若我们取$m=10$, 那么这意味着梯度更新加速了6000倍.

## 算法实现
我们对算法进行拆分地讲解, 首先需要创建一个神经网络类, 用来初始化神经网络:

{% highlight python %}
class Network:
	def __init__(self, sizes):
		self.num_layers = len(sizes)
		self.sizes = sizes
		self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
		self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]
{% endhighlight %}
举例来说, net = Network([2, 3, 1])代表初始化了一个三层网络, 第一层有2个神经元, 第二层有3个, 第三层有1个神经元. 

下一步计算网络输出,我们使用sigmoid函数:
{% highlight python %}
def sigmoid(z):
	return 1.0/(1.0 + np.exp(-z))
{% endhighlight %}
注意, z是一个Numpy array类型的向量, Numpy自动applies这个sigmoid函数elementwise.

再下一步我们为`Network`类添加一个`feedforward`方法, 作用是对给定的输入求出网络的输出. 然后将此方法对网络的每一层使用:
{% highlight python %}
def feedforward(self, a):
	for b, w in zip(self.biases, self.weights):
		a = sigmoid(np.dot(w, a) + b)
	return a
{% endhighlight %}

再次, 我们为`Network`类加入SGD方法:
{% highlight python %}
def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):
    """Train the neural network using mini-batch stochastic
    gradient descent.  The "training_data" is a list of tuples
    "(x, y)" representing the training inputs and the desired
    outputs.  The other non-optional parameters are
    self-explanatory.  If "test_data" is provided then the
    network will be evaluated against the test data after each
    epoch, and partial progress printed out.  This is useful for
    tracking progress, but slows things down substantially."""
    if test_data: n_test = len(test_data)
    n = len(training_data)
    for j in xrange(epochs):
        random.shuffle(training_data)
        mini_batches = [
            training_data[k:k+mini_batch_size]
            for k in xrange(0, n, mini_batch_size)]
        for mini_batch in mini_batches:
            self.update_mini_batch(mini_batch, eta)
        if test_data:
            print "Epoch {0}: {1} / {2}".format(
                j, self.evaluate(test_data), n_test)
        else:
            print "Epoch {0} complete".format(j)
{% endhighlight %}
`training_data`是一个由训练输入和对应的期望输出表征的二元组(x, y), `epoch`和`mini_batch_size`分别代表训练次数和训练集采样数量. `eta`是学习率, `test_data`是可选参数, 用来评价在每次训练后评价网络, 可以让我们看到网络的部分训练结果, 但会潜在地降低训练效率.

下面是`update_mini_batch`部分:
{% highlight python %}
def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The "mini_batch" is a list of tuples "(x, y)", and "eta"
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                       for b, nb in zip(self.biases, nabla_b)]
{% endhighlight %}
上述代码大部分工作量由`self.backprop(x, y)`完成, 实际上是调用了一个反向传播(backpropagation)的算法, 一种快速计算cost function梯度的算法. 因此`update_mini_batch`所做的就是对每个`mini_batch`中的训练样本惊醒梯度计算, 然后相应地更新`self.weights`和`self.biases`.

完整程序:
{% highlight python%}
"""
network.py
~~~~~~~~~~

A module to implement the stochastic gradient descent learning
algorithm for a feedforward neural network.  Gradients are calculated
using backpropagation.  Note that I have focused on making the code
simple, easily readable, and easily modifiable.  It is not optimized,
and omits many desirable features.
"""

#### Libraries
# Standard library
import random

# Third-party libraries
import numpy as np

class Network(object):

    def __init__(self, sizes):
        """The list ``sizes`` contains the number of neurons in the
        respective layers of the network.  For example, if the list
        was [2, 3, 1] then it would be a three-layer network, with the
        first layer containing 2 neurons, the second layer 3 neurons,
        and the third layer 1 neuron.  The biases and weights for the
        network are initialized randomly, using a Gaussian
        distribution with mean 0, and variance 1.  Note that the first
        layer is assumed to be an input layer, and by convention we
        won't set any biases for those neurons, since biases are only
        ever used in computing the outputs from later layers."""
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]

    def feedforward(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a

    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        """Train the neural network using mini-batch stochastic
        gradient descent.  The ``training_data`` is a list of tuples
        ``(x, y)`` representing the training inputs and the desired
        outputs.  The other non-optional parameters are
        self-explanatory.  If ``test_data`` is provided then the
        network will be evaluated against the test data after each
        epoch, and partial progress printed out.  This is useful for
        tracking progress, but slows things down substantially."""
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)

    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]

    def backprop(self, x, y):
        """Return a tuple ``(nabla_b, nabla_w)`` representing the
        gradient for the cost function C_x.  ``nabla_b`` and
        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar
        to ``self.biases`` and ``self.weights``."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
        # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        # Note that the variable l in the loop below is used a little
        # differently to the notation in Chapter 2 of the book.  Here,
        # l = 1 means the last layer of neurons, l = 2 is the
        # second-last layer, and so on.  It's a renumbering of the
        # scheme in the book, used here to take advantage of the fact
        # that Python can use negative indices in lists.
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)

    def evaluate(self, test_data):
        """Return the number of test inputs for which the neural
        network outputs the correct result. Note that the neural
        network's output is assumed to be the index of whichever
        neuron in the final layer has the highest activation."""
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)

    def cost_derivative(self, output_activations, y):
        """Return the vector of partial derivatives \partial C_x /
        \partial a for the output activations."""
        return (output_activations-y)

#### Miscellaneous functions
def sigmoid(z):
    """The sigmoid function."""
    return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
    """Derivative of the sigmoid function."""
    return sigmoid(z)*(1-sigmoid(z))
{% endhighlight %}

加载数据程序:
{% highlight python %}
"""
mnist_loader
~~~~~~~~~~~~

A library to load the MNIST image data.  For details of the data
structures that are returned, see the doc strings for ``load_data``
and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the
function usually called by our neural network code.
"""

#### Libraries
# Standard library
import cPickle
import gzip

# Third-party libraries
import numpy as np

def load_data():
    """Return the MNIST data as a tuple containing the training data,
    the validation data, and the test data.

    The ``training_data`` is returned as a tuple with two entries.
    The first entry contains the actual training images.  This is a
    numpy ndarray with 50,000 entries.  Each entry is, in turn, a
    numpy ndarray with 784 values, representing the 28 * 28 = 784
    pixels in a single MNIST image.

    The second entry in the ``training_data`` tuple is a numpy ndarray
    containing 50,000 entries.  Those entries are just the digit
    values (0...9) for the corresponding images contained in the first
    entry of the tuple.

    The ``validation_data`` and ``test_data`` are similar, except
    each contains only 10,000 images.

    This is a nice data format, but for use in neural networks it's
    helpful to modify the format of the ``training_data`` a little.
    That's done in the wrapper function ``load_data_wrapper()``, see
    below.
    """
    f = gzip.open('../data/mnist.pkl.gz', 'rb')
    training_data, validation_data, test_data = cPickle.load(f)
    f.close()
    return (training_data, validation_data, test_data)

def load_data_wrapper():
    """Return a tuple containing ``(training_data, validation_data,
    test_data)``. Based on ``load_data``, but the format is more
    convenient for use in our implementation of neural networks.

    In particular, ``training_data`` is a list containing 50,000
    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray
    containing the input image.  ``y`` is a 10-dimensional
    numpy.ndarray representing the unit vector corresponding to the
    correct digit for ``x``.

    ``validation_data`` and ``test_data`` are lists containing 10,000
    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional
    numpy.ndarry containing the input image, and ``y`` is the
    corresponding classification, i.e., the digit values (integers)
    corresponding to ``x``.

    Obviously, this means we're using slightly different formats for
    the training data and the validation / test data.  These formats
    turn out to be the most convenient for use in our neural network
    code."""
    tr_d, va_d, te_d = load_data()
    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]
    training_results = [vectorized_result(y) for y in tr_d[1]]
    training_data = zip(training_inputs, training_results)
    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]
    validation_data = zip(validation_inputs, va_d[1])
    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]
    test_data = zip(test_inputs, te_d[1])
    return (training_data, validation_data, test_data)

def vectorized_result(j):
    """Return a 10-dimensional unit vector with a 1.0 in the jth
    position and zeroes elsewhere.  This is used to convert a digit
    (0...9) into a corresponding desired output from the neural
    network."""
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e
{% endhighlight %}