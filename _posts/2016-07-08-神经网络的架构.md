---
layout: post
title: "神经网络的架构"
data: 2016-07-08 15:12:19
categories: Deeplearning
---
## Architecture
假设我们有下面的一个网络:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/archi.png)

我们把第一层叫`input layer`(输入层), 中间那一层叫`hidden layer`, 最后一层叫`output layer`. input layer和output layer都相当容易理解, 但hidden layer听起来可能有些神秘, 当我第一次看到这个词时, 甚至觉得带有某种深刻的哲学或数学意义. 但事实上, 它仅仅就是"不是输入层也不是输出层的网络层". 上图当中仅有一个隐藏层, 事实上其他网络可能有多个隐藏层. 由于历史的原因, 上图的多层网络有时被叫做 multilayer perceptrons或MLPs, 而忽略了它们是由sigmoid neurons构成的事实. 

输入和输出层往往是直观的, 例如, 我们要判断一个手写体的图片是否是数字"9". 一个自然的联想是将图片的像素密度编码为输入神经元的输入, 若图片是一张64 x 64的灰度图, 那么我们有4096=64 x 64个输入神经元, 像素密度在0到1之间. 输出层只有一个神经元, 当输出大于0.5时, 输入图像是"9", 否则不是"9".

虽然输入和输出层的设计是简单直观的, 但是隐藏层的设计相当有技巧. 隐藏层神经元的个数选择直接影响到计算量的大小, 但是好在研究者们开发出许多如何设计隐藏层的技巧. 直到现在, 我们都在讨论神经元的输出作为下一层神经元输入的网络, 我们称这种网络为前馈网络, feedforward neural network. 即网络中没有循环, 但其他类型的网络如recurrent neural network当中是存在这些循环的.

## A simple network to classify handwritten digits
我们使用一个三层网络:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/three.png)

上图网络的输入层由一张28 x 28个像素的图片编码构成, 每个像素的灰度值由0到1, 0代表白色, 1代表黑色. 隐藏层由15个神经元构成, 输出层由10个神经元构成, 如果第一个神经元被激发, 即输出约为1, 那么我们将这个图片识别为0.

你也许会思考这样一个问题, 为什么我们要使用10个神经元作为输出层. 毕竟, 这个网络的目标是告诉我们哪个数字代表了输入的图像. 一个看起来自然的方式是我们只需要4个二元输出神经元就可以表征所有10个数字, 因为4个神经元可以表示16种结果. 那么为什么我们还需要使用10个神经元. 事实上, 这个问题可以由实验证明, 我们同时使用两种方式来设计网络, 结果是使用10个输出神经元的效果要比4个更好. 

为了理解这个现象, 我们考虑使用10个神经元的结构. 我们考虑第一个输出神经元, 它根据隐藏层的加权输出来判断输入图片是否为"0". 那么隐藏层神经元是干嘛的? 好吧, 我们假设隐藏层的第一个神经元负责检测一张图片是否出现下面图片的部分:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/part.png)

那么, 隐藏层第一个神经元要做的事情就是赋予与上图位置重叠的像素极高的权重, 而对输入的其他像素位置赋予很小的权值. 类似地, 我们假设第二, 三, 四个隐藏层神经元是要检测如下图位置的像素是否存在:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/hidden.png)

正如你猜地那样, 这四个图像组合起来就是数字"0". 所以, 如果上面四个隐藏层神经元都被激发, 那么我们可以总结出输入的图像就是数字"0". 当然, 上面的四个证据组合并不唯一指向数字"0". 我们可以合法地得到更多其他的数字"0"的图像组合. 但是, 在此例子中, 我总结得到输入为数字"0"显然是安全的.

如果这就是神经网络的工作模式, 那么我们可以给出为什么10个输出神经元比4个要更好的解释. 假设我们只有4个输出神经元, 那么第一个输出神经元将要尝试决定输入图像的哪一个bit最重要, 这对于仅有4个输出神经元的网络是十分困难的.

## Learning with gradient descent
现在我们已经设计了一个神经网络, 那么它将如何通过学习来识别手写数字呢? 首先, 我们需要一个训练数据集, 这里我们使用MNIST data set. 数据集包含两部分, 第一部分包含60000张手写数字的图片, 作为训练集. 另一部分包含10000张图像, 作为测试集, 这些图像都是28 x 28的灰度图.

我们将训练输入用$x$来表示, 并且将$x$表示为一个$28 x 28 = 784$维的向量. 我们将输出表示为$y = y(x)$, $y$是一个10维的向量. 例如, 对于一个特定的训练图像, $x$, 对应数字6, 那么$y(x)=(0,0,0,0,0,0,1,0,0,0)^{T}$表示了相应的输出.

我们想实现这样一种算法, 即能够使我们找到合适的权值和偏置值来让神经网络对于所有训练输入$x$的输出逼近$y(x)$. 为了定量我们如何趋近这一目标, 我们定义一个cost function:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/cost.png)

在上式中, $w$表示网络权值的集合, $b$表示偏置集合, $n$是训练集的规模, $a$是当输入为$x$时的网络输出向量, 求和是针对所有训练输入. 我们的目标是使$C(w,b)$越小越好, 下面我们将使用梯度下降法(gradient descent)来达到这个目标.

假设$C$是一个带有$m$个变量的函数, $v_{1},...,v_m$. 那么$\Delta C$表示由权值$w$, 偏置值$b$构成的向量$v$的微小变化$\Delta v = (\Delta v_{1},...,\Delta v_m)^{T}$引起的cost变化:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/costchange.png)

cost的梯度:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/costchange.png)

对仅有两个变量的实例, 我们可以选择:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/changeforv.png)

这给了我们一种使$C$趋向最小值的方法, 通过不断重复使用对于$v$的更新规则:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/updatev.png)

因此我们可以将这个更新v的规则定义为`梯度下降法`(gradient descent algorithm). 它给我们提供了一种方法, 通过微小改变v的值来找到$C$的最小值. 一定程度上, 梯度下降法是搜索最小值的最优策略. 我们控制$v$的移动步长为$\|\Delta v\| = \epsilon$, $\epsilon$为选取的固定正数. 可以证明$\Delta v$使$\nabla C \centerdot \Delta v$减小的最优选择是$\Delta v = -\eta \nabla C$.

具体的更新策略是用$w_k$和$b_l$来代替$v_j$, 可以得到:


![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/weights.png)


重复使用这个更新策略, 我们就能使cost function渐渐趋向最小值. 但在实际应用梯度下降法时, 我们还会遇到一些挑战. 我们注意到cost function的形式: $C = \frac{1}{n}\sum_{x}C_x$, 是对单个训练样本的平均:$C_{x} = \frac{\| y(x) - a\|^2}{2}$. 实际上, 我们计算cost的梯度$\nabla C$时需要单独对每个训练样本计算cost的梯度$\nabla C_x$, 然后才有$\nabla C = \frac{1}{n}\sum_x \nabla C_x$. 然而训练样本规模往往很大, 这样的训练方法非常耗时, 学习过程很慢.


这时我们用随机梯度下降(stochastic gradient descent)来加速学习, 具体地就是随机采样整个训练输入, 然后在采样到的小规模训练样本上学习. 然后用得到的平均cost来近似总体cost:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/stochastic.png)

对$w_k$, $b_k$来说:

![image](https://github.com/ColdCodeCool/ColdCodeCool.github.io/raw/master/images/wb.png)

完成一次mini-batch的训练之后, 再对另一组mini-batch训练, 直到遍历完整个训练集, 然后我们就完成了一次epoch训练, 再进行下一个epoch的训练. 另外我们有必要注意cost function和mini-batch更新参数的规模变化, 上式中对cost乘以因子$\frac{1}{n}$. 有时我们忽略这个因子, 直接对每个训练样本cost求和, 这一点对训练样本规模未知(训练样本实时产生)的情况非常有用. 同样地, 对mini-batch更新时我们有时也忽略因子$\frac{1}{m}$. 从概念上这并不改变什么, 因为我们可以等效地调整$\eta$来实现. 举例来说, 假设总训练样本$n = 60000$, 若我们取$m=10$, 那么这意味着梯度更新加速了6000倍.

## 算法实现
我们对算法进行拆分地讲解, 首先需要创建一个神经网络类, 用来初始化神经网络:

{% highlight python %}
class Network:
	def __init__(self, sizes):
		self.num_layers = len(sizes)
		self.sizes = sizes
		self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
		self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]
{% endhighlight %}
举例来说, net = Network([2, 3, 1])代表初始化了一个三层网络, 第一层有2个神经元, 第二层有3个, 第三层有1个神经元. 

下一步计算网络输出,我们使用sigmoid函数:
{% highlight python %}
def sigmoid(z):
	return 1.0/(1.0 + np.exp(-z))
{% endhighlight %}
注意, z是一个Numpy array类型的向量, Numpy自动applies这个sigmoid函数elementwise.

再下一步我们为`Network`类添加一个`feedforward`方法, 作用是对给定的输入求出网络的输出. 然后将此方法对网络的每一层使用:
{% highlight python %}
def feedforward(self, a):
	for b, w in zip(self.biases, self.weights):
		a = sigmoid(np.dot(w, a) + b)
	return a
{% endhighlight %}

再次, 我们为`Network`类加入SGD方法:
